这篇论文题为《NOISE_STEP: Training in 1.58B With No Gradient Memory》，作者是Will Brickner。论文提出了一种新的算法，能够在**三元精度（1.58位）**下直接训练机器学习模型，而无需使用传统的反向传播（backpropagation）或动量（momentum）方法。该算法不仅能够显著减少训练过程中的内存和能量消耗，还可以与模型推理同时运行，成本相似。以下是论文的主要内容和解读：

### 1. **背景与动机**
   - **问题**：训练大型机器学习模型（如GPT-3）需要巨大的计算资源和能量消耗，成本极高，且只有少数机构能够承担。虽然低精度（如三元精度）推理已经被证明可以在不损失性能的情况下大幅提高推理速度和能效，但训练过程仍然需要高精度计算。
   - **目标**：本文提出了一种算法，能够在三元精度下直接训练模型，避免了传统的高精度训练方法，从而大幅减少内存和能量消耗。

### 2. **梯度估计**
   - **传统梯度计算**：传统的梯度计算通常使用反向传播，需要存储大量的中间结果，导致内存占用高。
   - **新方法**：本文提出了一种基于**雅可比向量积（Jacobian Vector Product, JVP）**的梯度估计方法。通过引入随机扰动向量 \(\nu\)，可以计算扰动与损失梯度的对齐（alignment），从而估计梯度。
   - **三元精度下的梯度估计**：在三元精度下，扰动向量 \(\nu\) 是稀疏的，且只需要对齐的符号信息来估计梯度。通过拒绝对齐幅度低于中位数的扰动，可以进一步提高收敛性。

### 3. **表示效率**
   - **伪随机噪声**：扰动向量 \(\nu\) 可以通过伪随机噪声生成，只需要一个种子（seed）即可重现，因此不需要存储或传输扰动向量，极大地减少了内存和通信开销。
   - **分布式训练**：由于梯度步骤只需要1.58位来表示，分布式训练中的通信开销大幅减少，适合大规模分布式训练。
   - **模型表示**：模型可以通过一系列紧凑的梯度步骤来表示，模型的大小不再直接依赖于参数数量，而是依赖于步骤和扰动的数量。例如，三元精度的GPT-3 175B模型可以压缩到600KB到19MB之间。

### 4. **收敛性**
   - **离散梯度步骤**：由于三元空间的离散性，梯度步骤也是离散的，权重轨迹不连续，损失曲线噪声较大。尽管如此，优化过程仍然能够像Adam优化器一样快速收敛。
   - **实验验证**：作者通过一个简单的多层感知机（MLP）在MNIST数据集上的实验，验证了该方法的收敛性。实验表明，虽然在高步数阶段收敛较慢，但通过调整噪声稀疏度可以改善收敛性。

### 5. **实现建议**
   - **三元编码**：传统的三元计算内核使用2位编码表示三元值，但存在27%的空间开销。作者建议使用更高效的编码方式，如每字节编码5个三元值，以减少内存占用。
   - **JVP稀疏性**：由于扰动向量是稀疏的，雅可比向量积的计算可以简化，减少计算量。
   - **扰动正交性**：正交的扰动向量能够提取更多的梯度信息，稀疏扰动向量已经具有较高的正交性，可以进一步优化存储和计算。

### 6. **相关工作**
   - 本文的算法受到了之前关于无反向传播梯度计算的工作的启发。虽然与本文不直接相关，但TemGrad算法也通过将高精度梯度量化为三元梯度来减少分布式学习中的通信开销。

### 7. **总结**
   - 本文提出了一种新的训练算法，能够在三元精度下直接训练模型，避免了传统的高精度训练方法，显著减少了内存和能量消耗。该方法特别适合分布式训练，且模型表示非常紧凑。尽管在高步数阶段收敛较慢，但通过调整噪声稀疏度可以改善收敛性。

### 关键贡献：
   - **三元精度训练**：首次提出在三元精度下直接训练模型，避免了传统的高精度训练。
   - **无反向传播**：通过雅可比向量积和随机扰动向量估计梯度，避免了反向传播的内存开销。
   - **分布式训练优化**：梯度步骤的紧凑表示大幅减少了分布式训练中的通信开销。
   - **模型压缩**：模型可以通过紧凑的梯度步骤表示，大幅减少了模型传输和存储的大小。

这篇论文为大规模机器学习模型的训练提供了一种新的思路，特别是在资源受限的环境下，具有重要的应用潜力。
